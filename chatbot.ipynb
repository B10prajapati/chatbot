{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Data Loading\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = []\n",
    "    classes = []\n",
    "    documents = []\n",
    "    ignore_words = ['?', '!']\n",
    "    data_file = open('intents.json').read()\n",
    "    intents = json.loads(data_file)\n",
    "    \n",
    "    # Data Preprocessing\n",
    "    for intent in intents['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            w = nltk.word_tokenize(pattern)\n",
    "            words.extend(w)\n",
    "            \n",
    "            documents.append((w, intent['tag']))\n",
    "            \n",
    "            if intent['tag'] not in classes:\n",
    "                classes.append(intent['tag'])\n",
    "    \n",
    "    words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "    words = sorted(list(set(words)))\n",
    "    \n",
    "    classes = sorted(list(set(classes)))\n",
    "    \n",
    "    print(f'{len(words)} words')\n",
    "    print(f'{len(classes)} classes')\n",
    "    print(f'{len(documents)} documents')\n",
    "    \n",
    "    pickle.dump(words, open('words.pkl', 'wb'))\n",
    "    pickle.dump(classes, open('classes.pkl', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "#print(output_empty)\n",
    "for doc in documents:\n",
    "    # bag of words (BOW)\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    \n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    \n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "    \n",
    "    #print(training)\n",
    "    #print(output_row)\n",
    "    #print(bag)\n",
    "    #print(pattern_words)\n",
    "    \n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# X-Pattern\n",
    "train_x = list(training[:,0])\n",
    "# Y-Intents\n",
    "train_y = list(training[:,1])\n",
    "print('Training data created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "print('model created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GUI PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "\n",
    "import tkinter\n",
    "from tkinter import Tk, Button, Text, Scrollbar\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    \n",
    "    bag = [0] * len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(f\"Foud in bag: {w}\")\n",
    "    return(np.array(bag))\n",
    "                    \n",
    "def predict_class(sentence, model):\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    \n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
    "    return return_list\n",
    "def get_response(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag'] == tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    res = get_response(ints, intents)\n",
    "    return res\n",
    "    \n",
    "    \n",
    "    \n",
    "def send():\n",
    "    msg = EntryBox.get('1.0', 'end-1c').strip()\n",
    "    EntryBox.delete('0.0', END)\n",
    "    \n",
    "    if msg != '':\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, 'You: ' + msg + '\\n\\n')\n",
    "        \n",
    "        res = chatbot_response(msg)\n",
    "        ChatLog.insert(END, 'Bot: ' + res + '\\n\\n')\n",
    "        \n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    model = load_model('chatbot_model.h5')\n",
    "    intents = json.loads(open('intents.json').read())\n",
    "    words = pickle.load(open('words.pkl', 'rb'))\n",
    "    classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "    \n",
    "    base = Tk()\n",
    "    base.title('Hello')\n",
    "    base.geometry('400x500')\n",
    "    base.resizable(width=FALSE, height=FALSE)\n",
    "\n",
    "    ChatLog = Text(base)\n",
    "    ChatLog.config(state=DISABLED)\n",
    "\n",
    "    scrollbar = Scrollbar(base)\n",
    "    ChatLog['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "    SendButton = Button(base, text='Send', command=send)\n",
    "\n",
    "    EntryBox = Text(base)\n",
    "\n",
    "    scrollbar.place(x=376, y=6, height=386)\n",
    "    ChatLog.place(x=6, y=6, height=386, width=370)\n",
    "    EntryBox.place(x=128, y=401, height=90, width=265)\n",
    "    SendButton.place(x=6, y=401, height=90)\n",
    "\n",
    "    base.mainloop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "\n",
    "def send():\n",
    "    msg = EntryBox.get('1.0', 'end-1c').strip()\n",
    "    EntryBox.delete('0.0', END)\n",
    "    \n",
    "    if msg != '':\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, 'You: ' + msg + '\\n\\n')\n",
    "        \n",
    "        res = chatbot_response(msg)\n",
    "        ChatLog.insert(END, 'Bot: ' + res + '\\n\\n')\n",
    "        \n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "\n",
    "base = Tk()\n",
    "base.title('Hello')\n",
    "base.geometry('400x500')\n",
    "base.resizable(width=FALSE, height=FALSE)\n",
    "\n",
    "ChatLog = Text(base)\n",
    "ChatLog.config(state=DISABLED)\n",
    "\n",
    "scrollbar = Scrollbar(base)\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "SendButton = Button(base, text='Send', command=send)\n",
    "\n",
    "EntryBox = Text(base)\n",
    "\n",
    "scrollbar.place(x=376, y=6, height=386)\n",
    "ChatLog.place(x=6, y=6, height=386, width=370)\n",
    "EntryBox.place(x=128, y=401, height=90, width=265)\n",
    "SendButton.place(x=6, y=401, height=90)\n",
    "\n",
    "base.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
